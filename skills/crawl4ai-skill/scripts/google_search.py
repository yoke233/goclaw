#!/usr/bin/env python3
"""
Google Search Scraper using Crawl4AI
Usage: python google_search.py "<search query>" [max_results]

Example: python google_search.py "2026å¹´Goè¯­è¨€å±•æœ›" 20
"""

import asyncio
import sys
import json
import urllib.parse
from typing import List, Dict

try:
    from crawl4ai.__version__ import __version__
    from packaging import version
    MIN_CRAWL4AI_VERSION = "0.7.4"
    if version.parse(__version__) < version.parse(MIN_CRAWL4AI_VERSION):
        print(f"âš ï¸  Warning: Crawl4AI {MIN_CRAWL4AI_VERSION}+ recommended (you have {__version__})")
except ImportError:
    print(f"â„¹ï¸  Crawl4AI {MIN_CRAWL4AI_VERSION}+ required")

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy


async def search_google_css(query: str, max_results: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ CSS é€‰æ‹©å™¨ç­–ç•¥æå– Google æœç´¢ç»“æœï¼ˆæœ€å¿«ï¼Œæ— éœ€ LLMï¼‰
    """

    # æ„å»ºæœç´¢ URL
    encoded_query = urllib.parse.quote(query)
    search_url = f"https://www.google.com/search?q={encoded_query}&num={max_results}"

    print(f"ğŸ” Searching: {query}")
    print(f"ğŸ“Š Max results: {max_results}")
    print(f"ğŸŒ URL: {search_url}")

    # å®šä¹‰ Google æœç´¢ç»“æœçš„ CSS schema
    # Google çš„ HTML ç»“æ„ä¼šå˜åŒ–ï¼Œè¿™é‡Œä½¿ç”¨å¸¸ç”¨çš„é€‰æ‹©å™¨
    schema = {
        "name": "search_results",
        "baseSelector": "div.g, div[data-hveid], div.tF2Cxc, div.yuRUbf",
        "fields": [
            {
                "name": "title",
                "selector": "h3, h3.LC20lb, div[role='heading']",
                "type": "text"
            },
            {
                "name": "link",
                "selector": "a",
                "type": "attribute",
                "attribute": "href"
            },
            {
                "name": "description",
                "selector": "div.VwiC3b, div.s, div.ITZIwc, span.aCOpRe",
                "type": "text"
            },
            {
                "name": "site_name",
                "selector": "div.NJo7tc, span.VuuXrf, cite",
                "type": "text"
            }
        ]
    }

    browser_config = BrowserConfig(
        headless=True,
        viewport_width=1920,
        viewport_height=1080,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    crawler_config = CrawlerRunConfig(
        extraction_strategy=JsonCssExtractionStrategy(schema=schema, verbose=True),
        cache_mode=CacheMode.BYPASS,
        wait_for="css:div.g, div.search, body",
        page_timeout=30000,
        js_code=[
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            "const waitFor = (ms) => new Promise(resolve => setTimeout(resolve, ms));",
            "await waitFor(2000);"
        ]
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=search_url, config=crawler_config)

        if result.success:
            print("âœ… Successfully fetched search results")

            if result.extracted_content:
                try:
                    data = json.loads(result.extracted_content)
                    # å¤„ç†åˆ—è¡¨å’Œå­—å…¸ä¸¤ç§æ ¼å¼
                    if isinstance(data, list):
                        results = data
                    else:
                        results = data.get("search_results", data.get("results", []))

                    # è¿‡æ»¤æ‰ç©ºç»“æœå’Œæ— æ•ˆç»“æœ
                    seen = set()
                    valid_results = []
                    for r in results:
                        if r.get("title") and r.get("link"):
                            # æ¸…ç† URLï¼ˆGoogle æœ‰æ—¶ä¼šåœ¨ URL å‰åŠ  /url?q=ï¼‰
                            link = r["link"]
                            if link.startswith("/url?q="):
                                from urllib.parse import urlparse, parse_qs
                                parsed = urlparse(link)
                                link = parse_qs(parsed.query).get("q", [link])[0]
                            r["link"] = link

                            # ä½¿ç”¨ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†å»é‡
                            if link not in seen:
                                seen.add(link)
                                valid_results.append(r)

                    print(f"ğŸ“‹ Extracted {len(valid_results)} valid results")
                    return valid_results[:max_results]

                except json.JSONDecodeError as e:
                    print(f"âŒ Failed to parse extracted content: {e}")
                    print("Raw output:", result.extracted_content[:500] if result.extracted_content else "None")
                    return []
            else:
                print("âš ï¸ No extracted content, trying alternative method...")
                return await search_google_llm_fallback(query, max_results)
        else:
            print(f"âŒ Failed: {result.error_message}")
            print("Trying fallback method...")
            return await search_google_llm_fallback(query, max_results)


async def search_google_llm_fallback(query: str, max_results: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ LLM ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆæå–æœç´¢ç»“æœ
    æ³¨æ„ï¼šè¿™éœ€è¦é…ç½® LLM API å¯†é’¥
    """

    print("ğŸ¤– Using LLM fallback extraction...")

    encoded_query = urllib.parse.quote(query)
    search_url = f"https://www.google.com/search?q={encoded_query}&num={max_results}"

    # å°è¯•ä½¿ç”¨ç®€å•çš„ LLM æå–
    extraction_strategy = LLMExtractionStrategy(
        provider="openai/gpt-4o-mini",
        instruction=f"""
        Extract the top {max_results} search results from this Google search page for "{query}".

        For each search result, extract:
        1. Title - the blue link text
        2. Link - the URL (clean the URL, remove /url?q= prefix if present)
        3. Description - the gray text snippet below the title
        4. Site name - the green text showing the website name

        Return as JSON with a "results" array containing objects with these fields.
        Skip any ads or sponsored content.
        """
    )

    crawler_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        cache_mode=CacheMode.BYPASS,
        page_timeout=30000
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=search_url, config=crawler_config)

        if result.success and result.extracted_content:
            try:
                data = json.loads(result.extracted_content)
                return data.get("results", [])
            except json.JSONDecodeError:
                print("âš ï¸ LLM output could not be parsed as JSON")
                return []
        else:
            print(f"âŒ Fallback also failed: {result.error_message}")
            return []


async def search_google_with_html_parsing(query: str, max_results: int = 20) -> List[Dict]:
    """
    ç›´æ¥è§£æ HTML ä½œä¸ºæœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
    """

    print("ğŸ”§ Using direct HTML parsing...")

    encoded_query = urllib.parse.quote(query)
    search_url = f"https://www.google.com/search?q={encoded_query}&num={max_results}"

    browser_config = BrowserConfig(
        headless=True,
        viewport_width=1920,
        viewport_height=1080,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        wait_for="css:body",
        page_timeout=30000,
        js_code=[
            "const waitFor = (ms) => new Promise(resolve => setTimeout(resolve, ms));",
            "await waitFor(3000);"
        ]
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=search_url, config=crawler_config)

        if result.success and result.html:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(result.html, 'html.parser')
            results = []

            # Google æœç´¢ç»“æœé€šå¸¸åœ¨ div.g ä¸­
            for div in soup.select('div.g, div.tF2Cxc'):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = div.select_one('h3')
                    title = title_elem.get_text() if title_elem else ""

                    # æå–é“¾æ¥
                    link_elem = div.select_one('a')
                    link = link_elem.get('href', '') if link_elem else ""

                    # æ¸…ç† Google é‡å®šå‘é“¾æ¥
                    if link.startswith('/url?q='):
                        from urllib.parse import urlparse, parse_qs, unquote
                        parsed = urlparse(link)
                        link = unquote(parse_qs(parsed.query).get('q', [link])[0])

                    # æå–æè¿°
                    desc_elem = div.select_one('div.VwiC3b, div.s, span.aCOpRe')
                    description = desc_elem.get_text() if desc_elem else ""

                    # æå–ç½‘ç«™åç§°
                    site_elem = div.select_one('div.NJo7tc, span.VuuXrf, cite')
                    site_name = site_elem.get_text() if site_elem else ""

                    if title and link and not link.startswith('#'):
                        results.append({
                            "title": title.strip(),
                            "link": link.strip(),
                            "description": description.strip(),
                            "site_name": site_name.strip()
                        })

                        if len(results) >= max_results:
                            break

                except Exception as e:
                    continue

            print(f"ğŸ“‹ Parsed {len(results)} results from HTML")
            return results
        else:
            print(f"âŒ HTML parsing failed")
            return []


async def main():
    if len(sys.argv) < 2:
        print("Usage: python google_search.py \"<search query>\" [max_results]")
        print("Example: python google_search.py \"2026å¹´Goè¯­è¨€å±•æœ›\" 20")
        sys.exit(1)

    query = sys.argv[1]
    max_results = int(sys.argv[2]) if len(sys.argv) > 2 else 20

    # æ–¹æ³•1: CSS æå–
    results = await search_google_css(query, max_results)

    # å¦‚æœ CSS æå–å¤±è´¥ï¼Œå°è¯• HTML è§£æ
    if not results:
        results = await search_google_with_html_parsing(query, max_results)

    # è¾“å‡ºç»“æœ
    if results:
        output = {
            "query": query,
            "total_results": len(results),
            "results": results
        }

        print("\n" + "="*60)
        print(f"âœ… Successfully extracted {len(results)} search results")
        print("="*60)

        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = "google_search_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ Results saved to: {output_file}")
        print("\nğŸ“‹ Preview (first 3 results):")
        print(json.dumps(results[:3], ensure_ascii=False, indent=2))

        # æ‰“å°å®Œæ•´ JSON åˆ° stdout
        print("\n" + "="*60)
        print("FULL JSON OUTPUT:")
        print("="*60)
        print(json.dumps(output, ensure_ascii=False, indent=2))
    else:
        print("âŒ No results extracted. Please check:")
        print("  1. Your internet connection")
        print("  2. Whether Google is blocking the request (try with headless=False)")
        print("  3. The CSS selectors (Google might have changed their HTML)")


if __name__ == "__main__":
    asyncio.run(main())
